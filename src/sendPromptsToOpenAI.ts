import * as fs from 'fs';
import * as dotenv from 'dotenv';
import { OpenAI } from 'openai';

dotenv.config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export type LLMResult = { // ì¼ë‹¨ json ê°ì²´ ë°˜í™˜ìœ¼ë¡œ
    step: string;
    filename: string;
    prompt: string;
    response: string;
};

export async function getLLMResponses(): Promise<LLMResult[]> {
    const promptsPath = 'llm_prompts.txt';
    if (!fs.existsSync(promptsPath)) {
        throw new Error('llm_prompts.txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.');
        
    }

    const prompts = fs.readFileSync(promptsPath, 'utf-8').split('\n\n---\n\n');
    const results: LLMResult[] = [];

    for (const [i, prompt] of prompts.entries()) {
    console.log(`ğŸ“¤ [${i + 1}]ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ ì „ì†¡ ì¤‘...`);
    try {
        const chat = await openai.chat.completions.create({
            model: 'gpt-3.5-turbo',
            messages: [
            {
                role: 'system',
                content: 'ë„ˆëŠ” GitHub Actions ë¡œê·¸ ë¶„ì„ ë„ìš°ë¯¸ì•¼. ì‹¤íŒ¨ ì´ìœ ë¥¼ ëª…í™•íˆ ì„¤ëª…í•´ì¤˜.',
            },
            { role: 'user', content: prompt },
        ],
        });

        const reply = chat.choices[0].message?.content ?? '';

        
        const stepMatch = prompt.match(/Step.+?"(.+?)"/);
        const step = stepMatch?.[1] ?? `Step ${i + 1}`;

        const fileMatch = prompt.match(/ë¡œê·¸ íŒŒì¼ "(.+?)"/);
        const filename = fileMatch?.[1] ?? `unknown_${i + 1}.txt`;

        results.push({
            step,
            filename,
            prompt,
            response: reply,
        });
    } catch (err) {
        console.error(`â— ì˜¤ë¥˜ ë°œìƒ (í”„ë¡¬í”„íŠ¸ ${i + 1}):`, err);
    }
    }

    return results;

}

